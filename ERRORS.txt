============================= test session starts ==============================
platform linux -- Python 3.7.3, pytest-6.0.1, py-1.9.0, pluggy-0.13.1
rootdir: /home/dacart/InnerEye-DeepLearning, configfile: pytest.ini
plugins: cov-2.10.1, forked-1.3.0, xdist-1.34.0
collected 128 items

Tests/ML/models/architectures/test_image_encoder_with_mlp.py .FFFFFFFFFF [  8%]
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF [ 64%]
FFFFFFFFFFFFF....................FFFFFFFFFFFF                            [100%]

=================================== FAILURES ===================================
_____ test_image_encoder[AggregationType.Average-None-None-True-1-8-False] _____

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:24Z INFO     Creating the default output folder structure.
2020-08-25T15:13:24Z INFO     Running outside of AzureML.
2020-08-25T15:13:24Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:24Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ScalarModelBase
2020-08-25T15:13:24Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ScalarModelBase/logs
2020-08-25T15:13:24Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:24Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:24Z INFO     Processing dataset (name=None)
2020-08-25T15:13:24Z INFO     Creating the default output folder structure.
2020-08-25T15:13:24Z INFO     Running outside of AzureML.
2020-08-25T15:13:24Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:24Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ImageEncoder
2020-08-25T15:13:24Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ImageEncoder/logs
2020-08-25T15:13:24Z INFO     Running outside of AzureML.
2020-08-25T15:13:24Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182
2020-08-25T15:13:24Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182/2020-08-25T151324Z_ImageEncoder
2020-08-25T15:13:24Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182/2020-08-25T151324Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151324Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182/2020-08-25T151324Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ef19337d2f54c4a85fd46d41e150182/2020-08-25T151324Z_ImageEncoder/logs
____ test_image_encoder[AggregationType.Average-None-None-True-0.1-1-True] _____

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0399, -0.0359,  0.1804, -0.1431,  0.1957,  0.0682],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:25Z INFO     Creating the default output folder structure.
2020-08-25T15:13:25Z INFO     Running outside of AzureML.
2020-08-25T15:13:25Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:25Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ScalarModelBase
2020-08-25T15:13:25Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ScalarModelBase/logs
2020-08-25T15:13:25Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:25Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:25Z INFO     Processing dataset (name=None)
2020-08-25T15:13:25Z INFO     Creating the default output folder structure.
2020-08-25T15:13:25Z INFO     Running outside of AzureML.
2020-08-25T15:13:25Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:25Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ImageEncoder
2020-08-25T15:13:25Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ImageEncoder/logs
2020-08-25T15:13:25Z INFO     Running outside of AzureML.
2020-08-25T15:13:25Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e
2020-08-25T15:13:25Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e/2020-08-25T151325Z_ImageEncoder
2020-08-25T15:13:25Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e/2020-08-25T151325Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151325Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e/2020-08-25T151325Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/691557ef18314793848d6f1c9a132c5e/2020-08-25T151325Z_ImageEncoder/logs
____ test_image_encoder[AggregationType.Average-None-None-True-0.1-1-False] ____

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:26Z INFO     Creating the default output folder structure.
2020-08-25T15:13:26Z INFO     Running outside of AzureML.
2020-08-25T15:13:26Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:26Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase
2020-08-25T15:13:26Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase/logs
2020-08-25T15:13:26Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:26Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:26Z INFO     Processing dataset (name=None)
2020-08-25T15:13:26Z INFO     Creating the default output folder structure.
2020-08-25T15:13:26Z INFO     Running outside of AzureML.
2020-08-25T15:13:26Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:26Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder
2020-08-25T15:13:26Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder/logs
2020-08-25T15:13:26Z INFO     Running outside of AzureML.
2020-08-25T15:13:26Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727
2020-08-25T15:13:26Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727/2020-08-25T151326Z_ImageEncoder
2020-08-25T15:13:26Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727/2020-08-25T151326Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727/2020-08-25T151326Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5c242c878ae94e959a360d9ce7c95727/2020-08-25T151326Z_ImageEncoder/logs
____ test_image_encoder[AggregationType.Average-None-None-True-0.5-4-True] _____

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2382,  0.1106,  0.1700, -0.2366,  0.2333, -0.1919, -0.0507],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:26Z INFO     Creating the default output folder structure.
2020-08-25T15:13:26Z INFO     Running outside of AzureML.
2020-08-25T15:13:26Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:26Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase
2020-08-25T15:13:26Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase/logs
2020-08-25T15:13:26Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:26Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:26Z INFO     Processing dataset (name=None)
2020-08-25T15:13:26Z INFO     Creating the default output folder structure.
2020-08-25T15:13:26Z INFO     Running outside of AzureML.
2020-08-25T15:13:26Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:26Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder
2020-08-25T15:13:26Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder/logs
2020-08-25T15:13:26Z INFO     Running outside of AzureML.
2020-08-25T15:13:26Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad
2020-08-25T15:13:26Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad/2020-08-25T151326Z_ImageEncoder
2020-08-25T15:13:26Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad/2020-08-25T151326Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151326Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad/2020-08-25T151326Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21dc6c25a6a54ad7877ad782b862f3ad/2020-08-25T151326Z_ImageEncoder/logs
____ test_image_encoder[AggregationType.Average-None-None-True-0.5-4-False] ____

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:27Z INFO     Creating the default output folder structure.
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase/logs
2020-08-25T15:13:27Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:27Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:27Z INFO     Processing dataset (name=None)
2020-08-25T15:13:27Z INFO     Creating the default output folder structure.
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder/logs
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf/2020-08-25T151327Z_ImageEncoder
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf/2020-08-25T151327Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf/2020-08-25T151327Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5523a9b8d574f0cbf95d9fb8b8e5baf/2020-08-25T151327Z_ImageEncoder/logs
_____ test_image_encoder[AggregationType.Average-None-None-False-1-0-True] _____

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1933, -0.1176, -0.1433,  0.3025], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:27Z INFO     Creating the default output folder structure.
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase/logs
2020-08-25T15:13:27Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:27Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:27Z INFO     Processing dataset (name=None)
2020-08-25T15:13:27Z INFO     Creating the default output folder structure.
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder/logs
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f/2020-08-25T151327Z_ImageEncoder
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f/2020-08-25T151327Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f/2020-08-25T151327Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/001c436f136a416cb22b9ce46585965f/2020-08-25T151327Z_ImageEncoder/logs
____ test_image_encoder[AggregationType.Average-None-None-False-1-0-False] _____

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:27Z INFO     Creating the default output folder structure.
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase/logs
2020-08-25T15:13:27Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:27Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:27Z INFO     Processing dataset (name=None)
2020-08-25T15:13:27Z INFO     Creating the default output folder structure.
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder/logs
2020-08-25T15:13:27Z INFO     Running outside of AzureML.
2020-08-25T15:13:27Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb
2020-08-25T15:13:27Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb/2020-08-25T151327Z_ImageEncoder
2020-08-25T15:13:27Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb/2020-08-25T151327Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151327Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb/2020-08-25T151327Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4d29fc50d95348f790fb3fb04f496abb/2020-08-25T151327Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 8
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1186,  0.1429, -0.0941, -0.0251,  0.1007, -0.1112,  0.2145, -0.1571,
         0.1059], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:28Z INFO     Creating the default output folder structure.
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase/logs
2020-08-25T15:13:28Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:28Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:28Z INFO     Processing dataset (name=None)
2020-08-25T15:13:28Z INFO     Creating the default output folder structure.
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder/logs
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3/2020-08-25T151328Z_ImageEncoder
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3/2020-08-25T151328Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3/2020-08-25T151328Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ca533b4362744ef388e25173b59633d3/2020-08-25T151328Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 8
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:28Z INFO     Creating the default output folder structure.
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase/logs
2020-08-25T15:13:28Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:28Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:28Z INFO     Processing dataset (name=None)
2020-08-25T15:13:28Z INFO     Creating the default output folder structure.
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder/logs
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc/2020-08-25T151328Z_ImageEncoder
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc/2020-08-25T151328Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc/2020-08-25T151328Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5ade46aab28c497ca15743929dccf8fc/2020-08-25T151328Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.1
expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1942, -0.1400,  0.0567,  0.1761, -0.2183,  0.1878],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:28Z INFO     Creating the default output folder structure.
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase/logs
2020-08-25T15:13:28Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:28Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:28Z INFO     Processing dataset (name=None)
2020-08-25T15:13:28Z INFO     Creating the default output folder structure.
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder/logs
2020-08-25T15:13:28Z INFO     Running outside of AzureML.
2020-08-25T15:13:28Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5
2020-08-25T15:13:28Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5/2020-08-25T151328Z_ImageEncoder
2020-08-25T15:13:28Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5/2020-08-25T151328Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151328Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5/2020-08-25T151328Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e85c833631534e6ba26d9001798f62a5/2020-08-25T151328Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.1
expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:29Z INFO     Creating the default output folder structure.
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase/logs
2020-08-25T15:13:29Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:29Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:29Z INFO     Processing dataset (name=None)
2020-08-25T15:13:29Z INFO     Creating the default output folder structure.
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder/logs
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115/2020-08-25T151329Z_ImageEncoder
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115/2020-08-25T151329Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115/2020-08-25T151329Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d28158660915491a97e97fdd0dd63115/2020-08-25T151329Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.5
expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0924, -0.2078, -0.1658,  0.1178, -0.2554,  0.1777, -0.1991],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:29Z INFO     Creating the default output folder structure.
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase/logs
2020-08-25T15:13:29Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:29Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:29Z INFO     Processing dataset (name=None)
2020-08-25T15:13:29Z INFO     Creating the default output folder structure.
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder/logs
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c/2020-08-25T151329Z_ImageEncoder
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c/2020-08-25T151329Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c/2020-08-25T151329Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/20cdf614de1a41f7bff17e57ce73b87c/2020-08-25T151329Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.5
expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:29Z INFO     Creating the default output folder structure.
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase/logs
2020-08-25T15:13:29Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:29Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:29Z INFO     Processing dataset (name=None)
2020-08-25T15:13:29Z INFO     Creating the default output folder structure.
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder/logs
2020-08-25T15:13:29Z INFO     Running outside of AzureML.
2020-08-25T15:13:29Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494
2020-08-25T15:13:29Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494/2020-08-25T151329Z_ImageEncoder
2020-08-25T15:13:29Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494/2020-08-25T151329Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151329Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494/2020-08-25T151329Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3aa248a1ff7046c2aeaae22dad616494/2020-08-25T151329Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1652, -0.1873,  0.0224,  0.1101], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:30Z INFO     Creating the default output folder structure.
2020-08-25T15:13:30Z INFO     Running outside of AzureML.
2020-08-25T15:13:30Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:30Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase
2020-08-25T15:13:30Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase/logs
2020-08-25T15:13:30Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:30Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:30Z INFO     Processing dataset (name=None)
2020-08-25T15:13:30Z INFO     Creating the default output folder structure.
2020-08-25T15:13:30Z INFO     Running outside of AzureML.
2020-08-25T15:13:30Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:30Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder
2020-08-25T15:13:30Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder/logs
2020-08-25T15:13:30Z INFO     Running outside of AzureML.
2020-08-25T15:13:30Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039
2020-08-25T15:13:30Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039/2020-08-25T151330Z_ImageEncoder
2020-08-25T15:13:30Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039/2020-08-25T151330Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039/2020-08-25T151330Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9967b0ae2dd94dd081c703d490bce039/2020-08-25T151330Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:30Z INFO     Creating the default output folder structure.
2020-08-25T15:13:30Z INFO     Running outside of AzureML.
2020-08-25T15:13:30Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:30Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase
2020-08-25T15:13:30Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase/logs
2020-08-25T15:13:30Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:30Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:30Z INFO     Processing dataset (name=None)
2020-08-25T15:13:30Z INFO     Creating the default output folder structure.
2020-08-25T15:13:30Z INFO     Running outside of AzureML.
2020-08-25T15:13:30Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:30Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder
2020-08-25T15:13:30Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder/logs
2020-08-25T15:13:30Z INFO     Running outside of AzureML.
2020-08-25T15:13:30Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7
2020-08-25T15:13:30Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7/2020-08-25T151330Z_ImageEncoder
2020-08-25T15:13:30Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7/2020-08-25T151330Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151330Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7/2020-08-25T151330Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b8e787a5ec4945638543de2fc970a3c7/2020-08-25T151330Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2254,  0.2213,  0.1116, -0.1372, -0.1674,  0.2217,  0.0010, -0.1380,
         0.1320], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:31Z INFO     Creating the default output folder structure.
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase/logs
2020-08-25T15:13:31Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:31Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:31Z INFO     Processing dataset (name=None)
2020-08-25T15:13:31Z INFO     Creating the default output folder structure.
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder/logs
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda/2020-08-25T151331Z_ImageEncoder
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda/2020-08-25T151331Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda/2020-08-25T151331Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7e62ef2560904dc185c6baa99b6a8eda/2020-08-25T151331Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:31Z INFO     Creating the default output folder structure.
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase/logs
2020-08-25T15:13:31Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:31Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:31Z INFO     Processing dataset (name=None)
2020-08-25T15:13:31Z INFO     Creating the default output folder structure.
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder/logs
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b/2020-08-25T151331Z_ImageEncoder
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b/2020-08-25T151331Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b/2020-08-25T151331Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cc82d6ec2c014973a385b7c6d6e54b0b/2020-08-25T151331Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0399, -0.0359,  0.1804, -0.1431,  0.1957,  0.0682],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:31Z INFO     Creating the default output folder structure.
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase/logs
2020-08-25T15:13:31Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:31Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:31Z INFO     Processing dataset (name=None)
2020-08-25T15:13:31Z INFO     Creating the default output folder structure.
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder/logs
2020-08-25T15:13:31Z INFO     Running outside of AzureML.
2020-08-25T15:13:31Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def
2020-08-25T15:13:31Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def/2020-08-25T151331Z_ImageEncoder
2020-08-25T15:13:31Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def/2020-08-25T151331Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151331Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def/2020-08-25T151331Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bed42d28a27243de954e6703c9249def/2020-08-25T151331Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:32Z INFO     Creating the default output folder structure.
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
2020-08-25T15:13:32Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Processing dataset (name=None)
2020-08-25T15:13:32Z INFO     Creating the default output folder structure.
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder/logs
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db/2020-08-25T151332Z_ImageEncoder
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db/2020-08-25T151332Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db/2020-08-25T151332Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ad880bb64bb4bd099843f78bf5508db/2020-08-25T151332Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2382,  0.1106,  0.1700, -0.2366,  0.2333, -0.1919, -0.0507],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:32Z INFO     Creating the default output folder structure.
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
2020-08-25T15:13:32Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Processing dataset (name=None)
2020-08-25T15:13:32Z INFO     Creating the default output folder structure.
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder/logs
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844/2020-08-25T151332Z_ImageEncoder
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844/2020-08-25T151332Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844/2020-08-25T151332Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3b8e53a521c3432e99e725afcf753844/2020-08-25T151332Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:32Z INFO     Creating the default output folder structure.
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
2020-08-25T15:13:32Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Processing dataset (name=None)
2020-08-25T15:13:32Z INFO     Creating the default output folder structure.
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder/logs
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3/2020-08-25T151332Z_ImageEncoder
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3/2020-08-25T151332Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3/2020-08-25T151332Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/02fdfe45cfc241f0bd6955e34323a5d3/2020-08-25T151332Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1933, -0.1176, -0.1433,  0.3025], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:32Z INFO     Creating the default output folder structure.
2020-08-25T15:13:32Z INFO     Running outside of AzureML.
2020-08-25T15:13:32Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:32Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
2020-08-25T15:13:32Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
2020-08-25T15:13:32Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:32Z INFO     Processing dataset (name=None)
2020-08-25T15:13:33Z INFO     Creating the default output folder structure.
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f/2020-08-25T151333Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151332Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4b7211140fdb4506954cf2d82ce27e9f/2020-08-25T151333Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:33Z INFO     Creating the default output folder structure.
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase/logs
2020-08-25T15:13:33Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:33Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:33Z INFO     Processing dataset (name=None)
2020-08-25T15:13:33Z INFO     Creating the default output folder structure.
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74/2020-08-25T151333Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2157bb830a404a058e1341ccefba1f74/2020-08-25T151333Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1186,  0.1429, -0.0941, -0.0251,  0.1007, -0.1112,  0.2145, -0.1571,
         0.1059], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:33Z INFO     Creating the default output folder structure.
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase/logs
2020-08-25T15:13:33Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:33Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:33Z INFO     Processing dataset (name=None)
2020-08-25T15:13:33Z INFO     Creating the default output folder structure.
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe/2020-08-25T151333Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b58b1b58a744709ae3ca1cf9f4984fe/2020-08-25T151333Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:33Z INFO     Creating the default output folder structure.
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase/logs
2020-08-25T15:13:33Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:33Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:33Z INFO     Processing dataset (name=None)
2020-08-25T15:13:33Z INFO     Creating the default output folder structure.
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
2020-08-25T15:13:33Z INFO     Running outside of AzureML.
2020-08-25T15:13:33Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03
2020-08-25T15:13:33Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03/2020-08-25T151333Z_ImageEncoder
2020-08-25T15:13:33Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03/2020-08-25T151333Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151333Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03/2020-08-25T151333Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e108512c824e456dba399635a4cf5f03/2020-08-25T151333Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1942, -0.1400,  0.0567,  0.1761, -0.2183,  0.1878],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:34Z INFO     Creating the default output folder structure.
2020-08-25T15:13:34Z INFO     Running outside of AzureML.
2020-08-25T15:13:34Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:34Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase
2020-08-25T15:13:34Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase/logs
2020-08-25T15:13:34Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:34Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:34Z INFO     Processing dataset (name=None)
2020-08-25T15:13:34Z INFO     Creating the default output folder structure.
2020-08-25T15:13:34Z INFO     Running outside of AzureML.
2020-08-25T15:13:34Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:34Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder
2020-08-25T15:13:34Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder/logs
2020-08-25T15:13:34Z INFO     Running outside of AzureML.
2020-08-25T15:13:34Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81
2020-08-25T15:13:34Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81/2020-08-25T151334Z_ImageEncoder
2020-08-25T15:13:34Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81/2020-08-25T151334Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81/2020-08-25T151334Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ac5b68e50ae54bedb6c027fa445dbb81/2020-08-25T151334Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:34Z INFO     Creating the default output folder structure.
2020-08-25T15:13:34Z INFO     Running outside of AzureML.
2020-08-25T15:13:34Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:34Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase
2020-08-25T15:13:34Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase/logs
2020-08-25T15:13:34Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:34Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:34Z INFO     Processing dataset (name=None)
2020-08-25T15:13:34Z INFO     Creating the default output folder structure.
2020-08-25T15:13:34Z INFO     Running outside of AzureML.
2020-08-25T15:13:34Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:34Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder
2020-08-25T15:13:34Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder/logs
2020-08-25T15:13:34Z INFO     Running outside of AzureML.
2020-08-25T15:13:34Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82
2020-08-25T15:13:34Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82/2020-08-25T151334Z_ImageEncoder
2020-08-25T15:13:34Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82/2020-08-25T151334Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151334Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82/2020-08-25T151334Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fa630821268a42fdbda8b7a63c45be82/2020-08-25T151334Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0924, -0.2078, -0.1658,  0.1178, -0.2554,  0.1777, -0.1991],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:35Z INFO     Creating the default output folder structure.
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase/logs
2020-08-25T15:13:35Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:35Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:35Z INFO     Processing dataset (name=None)
2020-08-25T15:13:35Z INFO     Creating the default output folder structure.
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder/logs
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8/2020-08-25T151335Z_ImageEncoder
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8/2020-08-25T151335Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8/2020-08-25T151335Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5a0dfe379ba448569e4815a8bc7a03b8/2020-08-25T151335Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:35Z INFO     Creating the default output folder structure.
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase/logs
2020-08-25T15:13:35Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:35Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:35Z INFO     Processing dataset (name=None)
2020-08-25T15:13:35Z INFO     Creating the default output folder structure.
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder/logs
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730/2020-08-25T151335Z_ImageEncoder
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730/2020-08-25T151335Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730/2020-08-25T151335Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4de7d83bb3a049ec8225090147241730/2020-08-25T151335Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1652, -0.1873,  0.0224,  0.1101], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:35Z INFO     Creating the default output folder structure.
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase/logs
2020-08-25T15:13:35Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:35Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:35Z INFO     Processing dataset (name=None)
2020-08-25T15:13:35Z INFO     Creating the default output folder structure.
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder/logs
2020-08-25T15:13:35Z INFO     Running outside of AzureML.
2020-08-25T15:13:35Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618
2020-08-25T15:13:35Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618/2020-08-25T151335Z_ImageEncoder
2020-08-25T15:13:35Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618/2020-08-25T151335Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151335Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618/2020-08-25T151335Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a8b1e20558eb4c098de3e82643cb8618/2020-08-25T151335Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.Average: 'Average'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:36Z INFO     Creating the default output folder structure.
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase/logs
2020-08-25T15:13:36Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:36Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:36Z INFO     Processing dataset (name=None)
2020-08-25T15:13:36Z INFO     Creating the default output folder structure.
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder/logs
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92/2020-08-25T151336Z_ImageEncoder
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92/2020-08-25T151336Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92/2020-08-25T151336Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b43535814d5d45ecb1301d91dd69ea92/2020-08-25T151336Z_ImageEncoder/logs
__ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-1-8-True] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2254,  0.2213,  0.1116, -0.1372, -0.1674,  0.2217,  0.0010, -0.1380,
         0.1320], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:36Z INFO     Creating the default output folder structure.
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase/logs
2020-08-25T15:13:36Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:36Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:36Z INFO     Processing dataset (name=None)
2020-08-25T15:13:36Z INFO     Creating the default output folder structure.
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder/logs
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee/2020-08-25T151336Z_ImageEncoder
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee/2020-08-25T151336Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee/2020-08-25T151336Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4cc829d034d144f2a2e583d7591d13ee/2020-08-25T151336Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-1-8-False] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:36Z INFO     Creating the default output folder structure.
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase/logs
2020-08-25T15:13:36Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:36Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:36Z INFO     Processing dataset (name=None)
2020-08-25T15:13:36Z INFO     Creating the default output folder structure.
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder/logs
2020-08-25T15:13:36Z INFO     Running outside of AzureML.
2020-08-25T15:13:36Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc
2020-08-25T15:13:36Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc/2020-08-25T151336Z_ImageEncoder
2020-08-25T15:13:36Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc/2020-08-25T151336Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151336Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc/2020-08-25T151336Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/403d2d8947024f409cff97fead2cd6bc/2020-08-25T151336Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0399, -0.0359,  0.1804, -0.1431,  0.1957,  0.0682],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:37Z INFO     Creating the default output folder structure.
2020-08-25T15:13:37Z INFO     Running outside of AzureML.
2020-08-25T15:13:37Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:37Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase
2020-08-25T15:13:37Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase/logs
2020-08-25T15:13:37Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:37Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:37Z INFO     Processing dataset (name=None)
2020-08-25T15:13:37Z INFO     Creating the default output folder structure.
2020-08-25T15:13:37Z INFO     Running outside of AzureML.
2020-08-25T15:13:37Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:37Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder
2020-08-25T15:13:37Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder/logs
2020-08-25T15:13:37Z INFO     Running outside of AzureML.
2020-08-25T15:13:37Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f
2020-08-25T15:13:37Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f/2020-08-25T151337Z_ImageEncoder
2020-08-25T15:13:37Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f/2020-08-25T151337Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f/2020-08-25T151337Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b01630a3a7804629b60656899e517b9f/2020-08-25T151337Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:37Z INFO     Creating the default output folder structure.
2020-08-25T15:13:37Z INFO     Running outside of AzureML.
2020-08-25T15:13:37Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:37Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase
2020-08-25T15:13:37Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase/logs
2020-08-25T15:13:37Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:37Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:37Z INFO     Processing dataset (name=None)
2020-08-25T15:13:37Z INFO     Creating the default output folder structure.
2020-08-25T15:13:37Z INFO     Running outside of AzureML.
2020-08-25T15:13:37Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:37Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder
2020-08-25T15:13:37Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder/logs
2020-08-25T15:13:37Z INFO     Running outside of AzureML.
2020-08-25T15:13:37Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b
2020-08-25T15:13:37Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b/2020-08-25T151337Z_ImageEncoder
2020-08-25T15:13:37Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b/2020-08-25T151337Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151337Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b/2020-08-25T151337Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/9f8e9c18d79544a3acfabe4ab113048b/2020-08-25T151337Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2382,  0.1106,  0.1700, -0.2366,  0.2333, -0.1919, -0.0507],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:38Z INFO     Creating the default output folder structure.
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase/logs
2020-08-25T15:13:38Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:38Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:38Z INFO     Processing dataset (name=None)
2020-08-25T15:13:38Z INFO     Creating the default output folder structure.
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder/logs
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622/2020-08-25T151338Z_ImageEncoder
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622/2020-08-25T151338Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622/2020-08-25T151338Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c6f7dd20e8004037973939c99368d622/2020-08-25T151338Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:38Z INFO     Creating the default output folder structure.
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase/logs
2020-08-25T15:13:38Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:38Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:38Z INFO     Processing dataset (name=None)
2020-08-25T15:13:38Z INFO     Creating the default output folder structure.
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder/logs
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98/2020-08-25T151338Z_ImageEncoder
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98/2020-08-25T151338Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98/2020-08-25T151338Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bb40f52fcdac49ad9f6b4ec62b06ca98/2020-08-25T151338Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-False-1-0-True] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1933, -0.1176, -0.1433,  0.3025], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:38Z INFO     Creating the default output folder structure.
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase/logs
2020-08-25T15:13:38Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:38Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:38Z INFO     Processing dataset (name=None)
2020-08-25T15:13:38Z INFO     Creating the default output folder structure.
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder/logs
2020-08-25T15:13:38Z INFO     Running outside of AzureML.
2020-08-25T15:13:38Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6
2020-08-25T15:13:38Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6/2020-08-25T151338Z_ImageEncoder
2020-08-25T15:13:38Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6/2020-08-25T151338Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151338Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6/2020-08-25T151338Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e8a46c704a7b44e7935dd45e695912e6/2020-08-25T151338Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:39Z INFO     Creating the default output folder structure.
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase/logs
2020-08-25T15:13:39Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:39Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:39Z INFO     Processing dataset (name=None)
2020-08-25T15:13:39Z INFO     Creating the default output folder structure.
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder/logs
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06/2020-08-25T151339Z_ImageEncoder
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06/2020-08-25T151339Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06/2020-08-25T151339Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/13ea09c4c95e45e7a2537d2c9a11fa06/2020-08-25T151339Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1186,  0.1429, -0.0941, -0.0251,  0.1007, -0.1112,  0.2145, -0.1571,
         0.1059], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:39Z INFO     Creating the default output folder structure.
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase/logs
2020-08-25T15:13:39Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:39Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:39Z INFO     Processing dataset (name=None)
2020-08-25T15:13:39Z INFO     Creating the default output folder structure.
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder/logs
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe/2020-08-25T151339Z_ImageEncoder
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe/2020-08-25T151339Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe/2020-08-25T151339Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0473e08c61f74186af1ac351d6b837fe/2020-08-25T151339Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:39Z INFO     Creating the default output folder structure.
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase/logs
2020-08-25T15:13:39Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:39Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:39Z INFO     Processing dataset (name=None)
2020-08-25T15:13:39Z INFO     Creating the default output folder structure.
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder/logs
2020-08-25T15:13:39Z INFO     Running outside of AzureML.
2020-08-25T15:13:39Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3
2020-08-25T15:13:39Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3/2020-08-25T151339Z_ImageEncoder
2020-08-25T15:13:39Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3/2020-08-25T151339Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151339Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3/2020-08-25T151339Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/7994ade5fa4047379747f2b38ef8dfa3/2020-08-25T151339Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.1
expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1942, -0.1400,  0.0567,  0.1761, -0.2183,  0.1878],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:40Z INFO     Creating the default output folder structure.
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase/logs
2020-08-25T15:13:40Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:40Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:40Z INFO     Processing dataset (name=None)
2020-08-25T15:13:40Z INFO     Creating the default output folder structure.
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder/logs
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7/2020-08-25T151340Z_ImageEncoder
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7/2020-08-25T151340Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7/2020-08-25T151340Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/335158c53dca4e8bbc2831baa80079d7/2020-08-25T151340Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.1
expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:40Z INFO     Creating the default output folder structure.
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase/logs
2020-08-25T15:13:40Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:40Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:40Z INFO     Processing dataset (name=None)
2020-08-25T15:13:40Z INFO     Creating the default output folder structure.
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder/logs
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8/2020-08-25T151340Z_ImageEncoder
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8/2020-08-25T151340Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8/2020-08-25T151340Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6d2a1a84e1474c1382b9bfbcbc6ca5b8/2020-08-25T151340Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.5
expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0924, -0.2078, -0.1658,  0.1178, -0.2554,  0.1777, -0.1991],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:40Z INFO     Creating the default output folder structure.
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase/logs
2020-08-25T15:13:40Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:40Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:40Z INFO     Processing dataset (name=None)
2020-08-25T15:13:40Z INFO     Creating the default output folder structure.
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder/logs
2020-08-25T15:13:40Z INFO     Running outside of AzureML.
2020-08-25T15:13:40Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d
2020-08-25T15:13:40Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d/2020-08-25T151340Z_ImageEncoder
2020-08-25T15:13:40Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d/2020-08-25T151340Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151340Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d/2020-08-25T151340Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1657fb1173164e9bbef49c821b75b08d/2020-08-25T151340Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.5
expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:41Z INFO     Creating the default output folder structure.
2020-08-25T15:13:41Z INFO     Running outside of AzureML.
2020-08-25T15:13:41Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:41Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase
2020-08-25T15:13:41Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase/logs
2020-08-25T15:13:41Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:41Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:41Z INFO     Processing dataset (name=None)
2020-08-25T15:13:41Z INFO     Creating the default output folder structure.
2020-08-25T15:13:41Z INFO     Running outside of AzureML.
2020-08-25T15:13:41Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:41Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder
2020-08-25T15:13:41Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder/logs
2020-08-25T15:13:41Z INFO     Running outside of AzureML.
2020-08-25T15:13:41Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244
2020-08-25T15:13:41Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244/2020-08-25T151341Z_ImageEncoder
2020-08-25T15:13:41Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244/2020-08-25T151341Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244/2020-08-25T151341Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2fc79ec1b8114da4ba655bccc23cb244/2020-08-25T151341Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1652, -0.1873,  0.0224,  0.1101], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:41Z INFO     Creating the default output folder structure.
2020-08-25T15:13:41Z INFO     Running outside of AzureML.
2020-08-25T15:13:41Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:41Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase
2020-08-25T15:13:41Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase/logs
2020-08-25T15:13:41Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:41Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:41Z INFO     Processing dataset (name=None)
2020-08-25T15:13:41Z INFO     Creating the default output folder structure.
2020-08-25T15:13:41Z INFO     Running outside of AzureML.
2020-08-25T15:13:41Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:41Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder
2020-08-25T15:13:41Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder/logs
2020-08-25T15:13:41Z INFO     Running outside of AzureML.
2020-08-25T15:13:41Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a
2020-08-25T15:13:41Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a/2020-08-25T151341Z_ImageEncoder
2020-08-25T15:13:41Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a/2020-08-25T151341Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151341Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a/2020-08-25T151341Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/97cd7e1bc220407bbfe6ce3e0030835a/2020-08-25T151341Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:42Z INFO     Creating the default output folder structure.
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase/logs
2020-08-25T15:13:42Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:42Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:42Z INFO     Processing dataset (name=None)
2020-08-25T15:13:42Z INFO     Creating the default output folder structure.
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder/logs
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d/2020-08-25T151342Z_ImageEncoder
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d/2020-08-25T151342Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d/2020-08-25T151342Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cd0bc02ffdc04c97b9b7800f26fef50d/2020-08-25T151342Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2254,  0.2213,  0.1116, -0.1372, -0.1674,  0.2217,  0.0010, -0.1380,
         0.1320], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:42Z INFO     Creating the default output folder structure.
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase/logs
2020-08-25T15:13:42Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:42Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:42Z INFO     Processing dataset (name=None)
2020-08-25T15:13:42Z INFO     Creating the default output folder structure.
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder/logs
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67/2020-08-25T151342Z_ImageEncoder
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67/2020-08-25T151342Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67/2020-08-25T151342Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8fcd9912e5f340d6802ff5d49b476c67/2020-08-25T151342Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:42Z INFO     Creating the default output folder structure.
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase/logs
2020-08-25T15:13:42Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:42Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:42Z INFO     Processing dataset (name=None)
2020-08-25T15:13:42Z INFO     Creating the default output folder structure.
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder/logs
2020-08-25T15:13:42Z INFO     Running outside of AzureML.
2020-08-25T15:13:42Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840
2020-08-25T15:13:42Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840/2020-08-25T151342Z_ImageEncoder
2020-08-25T15:13:42Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840/2020-08-25T151342Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151342Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840/2020-08-25T151342Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ef85eefe34c4f5b9b32ace0b7644840/2020-08-25T151342Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0399, -0.0359,  0.1804, -0.1431,  0.1957,  0.0682],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:43Z INFO     Creating the default output folder structure.
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase/logs
2020-08-25T15:13:43Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:43Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:43Z INFO     Processing dataset (name=None)
2020-08-25T15:13:43Z INFO     Creating the default output folder structure.
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder/logs
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6/2020-08-25T151343Z_ImageEncoder
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6/2020-08-25T151343Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6/2020-08-25T151343Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/b09553c62dd44ab59ef343b2cb0bc2c6/2020-08-25T151343Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:43Z INFO     Creating the default output folder structure.
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase/logs
2020-08-25T15:13:43Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:43Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:43Z INFO     Processing dataset (name=None)
2020-08-25T15:13:43Z INFO     Creating the default output folder structure.
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder/logs
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc/2020-08-25T151343Z_ImageEncoder
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc/2020-08-25T151343Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc/2020-08-25T151343Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd8020c551714b44887bb5bdd8a00ebc/2020-08-25T151343Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2382,  0.1106,  0.1700, -0.2366,  0.2333, -0.1919, -0.0507],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:43Z INFO     Creating the default output folder structure.
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase/logs
2020-08-25T15:13:43Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:43Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:43Z INFO     Processing dataset (name=None)
2020-08-25T15:13:43Z INFO     Creating the default output folder structure.
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder/logs
2020-08-25T15:13:43Z INFO     Running outside of AzureML.
2020-08-25T15:13:43Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7
2020-08-25T15:13:43Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7/2020-08-25T151343Z_ImageEncoder
2020-08-25T15:13:43Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7/2020-08-25T151343Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151343Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7/2020-08-25T151343Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62ac56d31cd44ac0afd45f235e9216d7/2020-08-25T151343Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:44Z INFO     Creating the default output folder structure.
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase/logs
2020-08-25T15:13:44Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:44Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:44Z INFO     Processing dataset (name=None)
2020-08-25T15:13:44Z INFO     Creating the default output folder structure.
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder/logs
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2/2020-08-25T151344Z_ImageEncoder
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2/2020-08-25T151344Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2/2020-08-25T151344Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3a17fa1c931e40e2af283a4177d4fee2/2020-08-25T151344Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1933, -0.1176, -0.1433,  0.3025], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:44Z INFO     Creating the default output folder structure.
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase/logs
2020-08-25T15:13:44Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:44Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:44Z INFO     Processing dataset (name=None)
2020-08-25T15:13:44Z INFO     Creating the default output folder structure.
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder/logs
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683/2020-08-25T151344Z_ImageEncoder
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683/2020-08-25T151344Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683/2020-08-25T151344Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/21f10d3bd83f4b60b61a8381bb3f2683/2020-08-25T151344Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:44Z INFO     Creating the default output folder structure.
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase/logs
2020-08-25T15:13:44Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:44Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:44Z INFO     Processing dataset (name=None)
2020-08-25T15:13:44Z INFO     Creating the default output folder structure.
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder/logs
2020-08-25T15:13:44Z INFO     Running outside of AzureML.
2020-08-25T15:13:44Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea
2020-08-25T15:13:44Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea/2020-08-25T151344Z_ImageEncoder
2020-08-25T15:13:44Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea/2020-08-25T151344Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151344Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea/2020-08-25T151344Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/409ac70c7f504527b93db63e18bbddea/2020-08-25T151344Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1186,  0.1429, -0.0941, -0.0251,  0.1007, -0.1112,  0.2145, -0.1571,
         0.1059], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:45Z INFO     Creating the default output folder structure.
2020-08-25T15:13:45Z INFO     Running outside of AzureML.
2020-08-25T15:13:45Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:45Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase
2020-08-25T15:13:45Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase/logs
2020-08-25T15:13:45Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:45Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:45Z INFO     Processing dataset (name=None)
2020-08-25T15:13:45Z INFO     Creating the default output folder structure.
2020-08-25T15:13:45Z INFO     Running outside of AzureML.
2020-08-25T15:13:45Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:45Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder
2020-08-25T15:13:45Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder/logs
2020-08-25T15:13:45Z INFO     Running outside of AzureML.
2020-08-25T15:13:45Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2
2020-08-25T15:13:45Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2/2020-08-25T151345Z_ImageEncoder
2020-08-25T15:13:45Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2/2020-08-25T151345Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2/2020-08-25T151345Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8d50eb9cc95148709eb319e6654971e2/2020-08-25T151345Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:45Z INFO     Creating the default output folder structure.
2020-08-25T15:13:45Z INFO     Running outside of AzureML.
2020-08-25T15:13:45Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:45Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase
2020-08-25T15:13:45Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase/logs
2020-08-25T15:13:45Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:45Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:45Z INFO     Processing dataset (name=None)
2020-08-25T15:13:45Z INFO     Creating the default output folder structure.
2020-08-25T15:13:45Z INFO     Running outside of AzureML.
2020-08-25T15:13:45Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:45Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder
2020-08-25T15:13:45Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder/logs
2020-08-25T15:13:45Z INFO     Running outside of AzureML.
2020-08-25T15:13:45Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8
2020-08-25T15:13:45Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8/2020-08-25T151345Z_ImageEncoder
2020-08-25T15:13:45Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8/2020-08-25T151345Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151345Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8/2020-08-25T151345Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/391c446f3e424245ab2a4493b6b504e8/2020-08-25T151345Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1942, -0.1400,  0.0567,  0.1761, -0.2183,  0.1878],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:46Z INFO     Creating the default output folder structure.
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase/logs
2020-08-25T15:13:46Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:46Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:46Z INFO     Processing dataset (name=None)
2020-08-25T15:13:46Z INFO     Creating the default output folder structure.
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder/logs
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1/2020-08-25T151346Z_ImageEncoder
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1/2020-08-25T151346Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1/2020-08-25T151346Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/76bc33e4d9634357a352ec5ff24b96f1/2020-08-25T151346Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:46Z INFO     Creating the default output folder structure.
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase/logs
2020-08-25T15:13:46Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:46Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:46Z INFO     Processing dataset (name=None)
2020-08-25T15:13:46Z INFO     Creating the default output folder structure.
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder/logs
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396/2020-08-25T151346Z_ImageEncoder
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396/2020-08-25T151346Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396/2020-08-25T151346Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f5411dd02ac84a52bd6e69ab52ea9396/2020-08-25T151346Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0924, -0.2078, -0.1658,  0.1178, -0.2554,  0.1777, -0.1991],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:46Z INFO     Creating the default output folder structure.
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase/logs
2020-08-25T15:13:46Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:46Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:46Z INFO     Processing dataset (name=None)
2020-08-25T15:13:46Z INFO     Creating the default output folder structure.
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder/logs
2020-08-25T15:13:46Z INFO     Running outside of AzureML.
2020-08-25T15:13:46Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f
2020-08-25T15:13:46Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f/2020-08-25T151346Z_ImageEncoder
2020-08-25T15:13:46Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f/2020-08-25T151346Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151346Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f/2020-08-25T151346Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0b11bd31e3814c26b6defef505dd491f/2020-08-25T151346Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:47Z INFO     Creating the default output folder structure.
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase/logs
2020-08-25T15:13:47Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:47Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:47Z INFO     Processing dataset (name=None)
2020-08-25T15:13:47Z INFO     Creating the default output folder structure.
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder/logs
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a/2020-08-25T151347Z_ImageEncoder
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a/2020-08-25T151347Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a/2020-08-25T151347Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c9a33fbce29845bd9d7aa02ab39ab75a/2020-08-25T151347Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1652, -0.1873,  0.0224,  0.1101], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:47Z INFO     Creating the default output folder structure.
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase/logs
2020-08-25T15:13:47Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:47Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:47Z INFO     Processing dataset (name=None)
2020-08-25T15:13:47Z INFO     Creating the default output folder structure.
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder/logs
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587/2020-08-25T151347Z_ImageEncoder
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587/2020-08-25T151347Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587/2020-08-25T151347Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/45116ba8f3d943e7abf651f053464587/2020-08-25T151347Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.ZAdaptive3dAvg: 'Adaptive3dAverage'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:47Z INFO     Creating the default output folder structure.
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase/logs
2020-08-25T15:13:47Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:47Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:47Z INFO     Processing dataset (name=None)
2020-08-25T15:13:47Z INFO     Creating the default output folder structure.
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder/logs
2020-08-25T15:13:47Z INFO     Running outside of AzureML.
2020-08-25T15:13:47Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290
2020-08-25T15:13:47Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290/2020-08-25T151347Z_ImageEncoder
2020-08-25T15:13:47Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290/2020-08-25T151347Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151347Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290/2020-08-25T151347Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/f13f76af76a546608f9774d061b8d290/2020-08-25T151347Z_ImageEncoder/logs
___ test_image_encoder[AggregationType.GatedPooling-None-None-True-1-8-True] ___

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.0998, -0.0565,  0.0657, -0.2213,  0.1740,  0.1650,  0.0972,  0.1717,
        -0.0714], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:48Z INFO     Creating the default output folder structure.
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase/logs
2020-08-25T15:13:48Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:48Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:48Z INFO     Processing dataset (name=None)
2020-08-25T15:13:48Z INFO     Creating the default output folder structure.
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder/logs
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c/2020-08-25T151348Z_ImageEncoder
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c/2020-08-25T151348Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c/2020-08-25T151348Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/0a9744a79f944aa78814420fecb1810c/2020-08-25T151348Z_ImageEncoder/logs
__ test_image_encoder[AggregationType.GatedPooling-None-None-True-1-8-False] ___

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:48Z INFO     Creating the default output folder structure.
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase/logs
2020-08-25T15:13:48Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:48Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:48Z INFO     Processing dataset (name=None)
2020-08-25T15:13:48Z INFO     Creating the default output folder structure.
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder/logs
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d/2020-08-25T151348Z_ImageEncoder
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d/2020-08-25T151348Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d/2020-08-25T151348Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1ee87371e63f49c5bd3e230223500b7d/2020-08-25T151348Z_ImageEncoder/logs
__ test_image_encoder[AggregationType.GatedPooling-None-None-True-0.1-1-True] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0376, -0.2818,  0.0769, -0.2659, -0.2009, -0.0599],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:48Z INFO     Creating the default output folder structure.
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase/logs
2020-08-25T15:13:48Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:48Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:48Z INFO     Processing dataset (name=None)
2020-08-25T15:13:48Z INFO     Creating the default output folder structure.
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder/logs
2020-08-25T15:13:48Z INFO     Running outside of AzureML.
2020-08-25T15:13:48Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c
2020-08-25T15:13:48Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c/2020-08-25T151348Z_ImageEncoder
2020-08-25T15:13:48Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c/2020-08-25T151348Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151348Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c/2020-08-25T151348Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2088c13e2ffe4555963448f81360c39c/2020-08-25T151348Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-None-True-0.1-1-False] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:49Z INFO     Creating the default output folder structure.
2020-08-25T15:13:49Z INFO     Running outside of AzureML.
2020-08-25T15:13:49Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:49Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase
2020-08-25T15:13:49Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase/logs
2020-08-25T15:13:49Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:49Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:49Z INFO     Processing dataset (name=None)
2020-08-25T15:13:49Z INFO     Creating the default output folder structure.
2020-08-25T15:13:49Z INFO     Running outside of AzureML.
2020-08-25T15:13:49Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:49Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder
2020-08-25T15:13:49Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder/logs
2020-08-25T15:13:49Z INFO     Running outside of AzureML.
2020-08-25T15:13:49Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef
2020-08-25T15:13:49Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef/2020-08-25T151349Z_ImageEncoder
2020-08-25T15:13:49Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef/2020-08-25T151349Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef/2020-08-25T151349Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/287cabf035704fbc8d99420b0ec632ef/2020-08-25T151349Z_ImageEncoder/logs
__ test_image_encoder[AggregationType.GatedPooling-None-None-True-0.5-4-True] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1985,  0.1368,  0.2063, -0.1395,  0.0968,  0.0792,  0.1008],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:49Z INFO     Creating the default output folder structure.
2020-08-25T15:13:49Z INFO     Running outside of AzureML.
2020-08-25T15:13:49Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:49Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase
2020-08-25T15:13:49Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase/logs
2020-08-25T15:13:49Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:49Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:49Z INFO     Processing dataset (name=None)
2020-08-25T15:13:49Z INFO     Creating the default output folder structure.
2020-08-25T15:13:49Z INFO     Running outside of AzureML.
2020-08-25T15:13:49Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:49Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder
2020-08-25T15:13:49Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder/logs
2020-08-25T15:13:49Z INFO     Running outside of AzureML.
2020-08-25T15:13:49Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4
2020-08-25T15:13:49Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4/2020-08-25T151349Z_ImageEncoder
2020-08-25T15:13:49Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4/2020-08-25T151349Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151349Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4/2020-08-25T151349Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fe364e5078b3458cb8dc4eafca47ecc4/2020-08-25T151349Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-None-True-0.5-4-False] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:50Z INFO     Creating the default output folder structure.
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase/logs
2020-08-25T15:13:50Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:50Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:50Z INFO     Processing dataset (name=None)
2020-08-25T15:13:50Z INFO     Creating the default output folder structure.
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder/logs
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d/2020-08-25T151350Z_ImageEncoder
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d/2020-08-25T151350Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d/2020-08-25T151350Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8703816288bc4ce8803afb7a6998152d/2020-08-25T151350Z_ImageEncoder/logs
__ test_image_encoder[AggregationType.GatedPooling-None-None-False-1-0-True] ___

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2444,  0.0082,  0.1133, -0.0074], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:50Z INFO     Creating the default output folder structure.
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase/logs
2020-08-25T15:13:50Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:50Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:50Z INFO     Processing dataset (name=None)
2020-08-25T15:13:50Z INFO     Creating the default output folder structure.
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder/logs
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3/2020-08-25T151350Z_ImageEncoder
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3/2020-08-25T151350Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3/2020-08-25T151350Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/98d11dee32b9411d8e6e859cfde12df3/2020-08-25T151350Z_ImageEncoder/logs
__ test_image_encoder[AggregationType.GatedPooling-None-None-False-1-0-False] __

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = None, stride_size_per_encoding_block = None
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:50Z INFO     Creating the default output folder structure.
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase/logs
2020-08-25T15:13:50Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:50Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:50Z INFO     Processing dataset (name=None)
2020-08-25T15:13:50Z INFO     Creating the default output folder structure.
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder/logs
2020-08-25T15:13:50Z INFO     Running outside of AzureML.
2020-08-25T15:13:50Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab
2020-08-25T15:13:50Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab/2020-08-25T151350Z_ImageEncoder
2020-08-25T15:13:50Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab/2020-08-25T151350Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151350Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab/2020-08-25T151350Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/6cdf84828aa741eea661b7c59bc0a9ab/2020-08-25T151350Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1557,  0.0940,  0.0265,  0.0770, -0.0571, -0.1083,  0.1883,  0.0769,
        -0.0931], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:51Z INFO     Creating the default output folder structure.
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase/logs
2020-08-25T15:13:51Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:51Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:51Z INFO     Processing dataset (name=None)
2020-08-25T15:13:51Z INFO     Creating the default output folder structure.
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder/logs
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb/2020-08-25T151351Z_ImageEncoder
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb/2020-08-25T151351Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb/2020-08-25T151351Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/18e9993eb18f45cbb6c7ed7f7d2ff6fb/2020-08-25T151351Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:51Z INFO     Creating the default output folder structure.
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase/logs
2020-08-25T15:13:51Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:51Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:51Z INFO     Processing dataset (name=None)
2020-08-25T15:13:51Z INFO     Creating the default output folder structure.
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder/logs
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac/2020-08-25T151351Z_ImageEncoder
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac/2020-08-25T151351Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac/2020-08-25T151351Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/328cb4d73a3746feb2157933f4f744ac/2020-08-25T151351Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.1
expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1887, -0.0078,  0.2787,  0.2307,  0.2199, -0.0285],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:51Z INFO     Creating the default output folder structure.
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase/logs
2020-08-25T15:13:51Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:51Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:51Z INFO     Processing dataset (name=None)
2020-08-25T15:13:51Z INFO     Creating the default output folder structure.
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder/logs
2020-08-25T15:13:51Z INFO     Running outside of AzureML.
2020-08-25T15:13:51Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398
2020-08-25T15:13:51Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398/2020-08-25T151351Z_ImageEncoder
2020-08-25T15:13:51Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398/2020-08-25T151351Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151351Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398/2020-08-25T151351Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/15fa36f6f0634cdfb18b7378827f8398/2020-08-25T151351Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.1
expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:52Z INFO     Creating the default output folder structure.
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase/logs
2020-08-25T15:13:52Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:52Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:52Z INFO     Processing dataset (name=None)
2020-08-25T15:13:52Z INFO     Creating the default output folder structure.
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder/logs
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b/2020-08-25T151352Z_ImageEncoder
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b/2020-08-25T151352Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b/2020-08-25T151352Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/bec5d54cf7704b8390e11f40af1cf75b/2020-08-25T151352Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.5
expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0737, -0.2026, -0.2026, -0.0160, -0.0324,  0.1917,  0.1875],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:52Z INFO     Creating the default output folder structure.
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase/logs
2020-08-25T15:13:52Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:52Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:52Z INFO     Processing dataset (name=None)
2020-08-25T15:13:52Z INFO     Creating the default output folder structure.
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder/logs
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313/2020-08-25T151352Z_ImageEncoder
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313/2020-08-25T151352Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313/2020-08-25T151352Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/e03dde7fb5204124a30a3f2404ebc313/2020-08-25T151352Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 0.5
expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:52Z INFO     Creating the default output folder structure.
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase/logs
2020-08-25T15:13:52Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:52Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:52Z INFO     Processing dataset (name=None)
2020-08-25T15:13:52Z INFO     Creating the default output folder structure.
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder/logs
2020-08-25T15:13:52Z INFO     Running outside of AzureML.
2020-08-25T15:13:52Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62
2020-08-25T15:13:52Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62/2020-08-25T151352Z_ImageEncoder
2020-08-25T15:13:52Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62/2020-08-25T151352Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151352Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62/2020-08-25T151352Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/16887065d3ef4106af18b54993a10c62/2020-08-25T151352Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2544,  0.0551, -0.0494,  0.0172], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:53Z INFO     Creating the default output folder structure.
2020-08-25T15:13:53Z INFO     Running outside of AzureML.
2020-08-25T15:13:53Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:53Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase
2020-08-25T15:13:53Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase/logs
2020-08-25T15:13:53Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:53Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:53Z INFO     Processing dataset (name=None)
2020-08-25T15:13:53Z INFO     Creating the default output folder structure.
2020-08-25T15:13:53Z INFO     Running outside of AzureML.
2020-08-25T15:13:53Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:53Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder
2020-08-25T15:13:53Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder/logs
2020-08-25T15:13:53Z INFO     Running outside of AzureML.
2020-08-25T15:13:53Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320
2020-08-25T15:13:53Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320/2020-08-25T151353Z_ImageEncoder
2020-08-25T15:13:53Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320/2020-08-25T151353Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320/2020-08-25T151353Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/1559dc5c2c1341bc96416ed7a3ff2320/2020-08-25T151353Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = None, reduction_factor = 1
expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:53Z INFO     Creating the default output folder structure.
2020-08-25T15:13:53Z INFO     Running outside of AzureML.
2020-08-25T15:13:53Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:53Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase
2020-08-25T15:13:53Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase/logs
2020-08-25T15:13:53Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:53Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:53Z INFO     Processing dataset (name=None)
2020-08-25T15:13:53Z INFO     Creating the default output folder structure.
2020-08-25T15:13:53Z INFO     Running outside of AzureML.
2020-08-25T15:13:53Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:53Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder
2020-08-25T15:13:53Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder/logs
2020-08-25T15:13:53Z INFO     Running outside of AzureML.
2020-08-25T15:13:53Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f
2020-08-25T15:13:53Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f/2020-08-25T151353Z_ImageEncoder
2020-08-25T15:13:53Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f/2020-08-25T151353Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151353Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f/2020-08-25T151353Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/642b553c31494c6f8a2c2b6072a6b59f/2020-08-25T151353Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2029, -0.1491, -0.1784,  0.2194,  0.0557, -0.1658, -0.1995,  0.1700,
         0.2107], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:54Z INFO     Creating the default output folder structure.
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase/logs
2020-08-25T15:13:54Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:54Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:54Z INFO     Processing dataset (name=None)
2020-08-25T15:13:54Z INFO     Creating the default output folder structure.
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder/logs
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409/2020-08-25T151354Z_ImageEncoder
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409/2020-08-25T151354Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409/2020-08-25T151354Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dcf718bae8524940885d241f3a1e7409/2020-08-25T151354Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:54Z INFO     Creating the default output folder structure.
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase/logs
2020-08-25T15:13:54Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:54Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:54Z INFO     Processing dataset (name=None)
2020-08-25T15:13:54Z INFO     Creating the default output folder structure.
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder/logs
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2/2020-08-25T151354Z_ImageEncoder
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2/2020-08-25T151354Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2/2020-08-25T151354Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/3ae3d18931444c929c9c11a76b6b87d2/2020-08-25T151354Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0922, -0.0062,  0.2244,  0.0421, -0.2838, -0.2384],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:54Z INFO     Creating the default output folder structure.
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase/logs
2020-08-25T15:13:54Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:54Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:54Z INFO     Processing dataset (name=None)
2020-08-25T15:13:54Z INFO     Creating the default output folder structure.
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder/logs
2020-08-25T15:13:54Z INFO     Running outside of AzureML.
2020-08-25T15:13:54Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f
2020-08-25T15:13:54Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f/2020-08-25T151354Z_ImageEncoder
2020-08-25T15:13:54Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f/2020-08-25T151354Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151354Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f/2020-08-25T151354Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a5bab0f46d0344dd8c6f451f5497ef9f/2020-08-25T151354Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:55Z INFO     Creating the default output folder structure.
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase/logs
2020-08-25T15:13:55Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:55Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:55Z INFO     Processing dataset (name=None)
2020-08-25T15:13:55Z INFO     Creating the default output folder structure.
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder/logs
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd/2020-08-25T151355Z_ImageEncoder
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd/2020-08-25T151355Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd/2020-08-25T151355Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/32fbc13e264442f4a433dea44d4d61bd/2020-08-25T151355Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([0.0041, 0.0403, 0.0861, 0.1981, 0.1486, 0.0330, 0.1032],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:55Z INFO     Creating the default output folder structure.
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase/logs
2020-08-25T15:13:55Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:55Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:55Z INFO     Processing dataset (name=None)
2020-08-25T15:13:55Z INFO     Creating the default output folder structure.
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder/logs
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd/2020-08-25T151355Z_ImageEncoder
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd/2020-08-25T151355Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd/2020-08-25T151355Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c4be0958dc9e476f89064c6590eb06cd/2020-08-25T151355Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:55Z INFO     Creating the default output folder structure.
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase/logs
2020-08-25T15:13:55Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:55Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:55Z INFO     Processing dataset (name=None)
2020-08-25T15:13:55Z INFO     Creating the default output folder structure.
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder/logs
2020-08-25T15:13:55Z INFO     Running outside of AzureML.
2020-08-25T15:13:55Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448
2020-08-25T15:13:55Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448/2020-08-25T151355Z_ImageEncoder
2020-08-25T15:13:55Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448/2020-08-25T151355Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151355Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448/2020-08-25T151355Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/4ea5292c12404d08a1087878b7135448/2020-08-25T151355Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.0160, -0.1014,  0.2807, -0.0185], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:56Z INFO     Creating the default output folder structure.
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase/logs
2020-08-25T15:13:56Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:56Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:56Z INFO     Processing dataset (name=None)
2020-08-25T15:13:56Z INFO     Creating the default output folder structure.
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder/logs
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1/2020-08-25T151356Z_ImageEncoder
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1/2020-08-25T151356Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1/2020-08-25T151356Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/c08f75332a684a30b6575d52f5e8d0a1/2020-08-25T151356Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = None
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2073,  0.0530,  0.3021],
           [ 0.2080, -0.1018, -0.3500],
           [ 0.2...1544, -0.3586],
           [-0.7538,  0.0087, -0.3538],
           [ 0.0874,  0.2928,  0.3008]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:56Z INFO     Creating the default output folder structure.
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase/logs
2020-08-25T15:13:56Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:56Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:56Z INFO     Processing dataset (name=None)
2020-08-25T15:13:56Z INFO     Creating the default output folder structure.
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder/logs
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace/2020-08-25T151356Z_ImageEncoder
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace/2020-08-25T151356Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace/2020-08-25T151356Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/072b2c271ac141a2b1b94e382d14cace/2020-08-25T151356Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.1185, -0.1249,  0.0602,  0.0236,  0.0163, -0.1246,  0.1606,  0.0330,
         0.0852], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:56Z INFO     Creating the default output folder structure.
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase/logs
2020-08-25T15:13:56Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:56Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:56Z INFO     Processing dataset (name=None)
2020-08-25T15:13:56Z INFO     Creating the default output folder structure.
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder/logs
2020-08-25T15:13:56Z INFO     Running outside of AzureML.
2020-08-25T15:13:56Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5
2020-08-25T15:13:56Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5/2020-08-25T151356Z_ImageEncoder
2020-08-25T15:13:56Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5/2020-08-25T151356Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151356Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5/2020-08-25T151356Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26c1f6316b7b4dcf8ebec54d982859c5/2020-08-25T151356Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 8
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:57Z INFO     Creating the default output folder structure.
2020-08-25T15:13:57Z INFO     Running outside of AzureML.
2020-08-25T15:13:57Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:57Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase
2020-08-25T15:13:57Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase/logs
2020-08-25T15:13:57Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:57Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:57Z INFO     Processing dataset (name=None)
2020-08-25T15:13:57Z INFO     Creating the default output folder structure.
2020-08-25T15:13:57Z INFO     Running outside of AzureML.
2020-08-25T15:13:57Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:57Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder
2020-08-25T15:13:57Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder/logs
2020-08-25T15:13:57Z INFO     Running outside of AzureML.
2020-08-25T15:13:57Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7
2020-08-25T15:13:57Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7/2020-08-25T151357Z_ImageEncoder
2020-08-25T15:13:57Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7/2020-08-25T151357Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7/2020-08-25T151357Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/027a95ab55474db58a5f5aba83549cb7/2020-08-25T151357Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.0512, -0.1263,  0.0909, -0.0567,  0.1381,  0.0732],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:57Z INFO     Creating the default output folder structure.
2020-08-25T15:13:57Z INFO     Running outside of AzureML.
2020-08-25T15:13:57Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:57Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase
2020-08-25T15:13:57Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase/logs
2020-08-25T15:13:57Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:57Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:57Z INFO     Processing dataset (name=None)
2020-08-25T15:13:57Z INFO     Creating the default output folder structure.
2020-08-25T15:13:57Z INFO     Running outside of AzureML.
2020-08-25T15:13:57Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:57Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder
2020-08-25T15:13:57Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder/logs
2020-08-25T15:13:57Z INFO     Running outside of AzureML.
2020-08-25T15:13:57Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62
2020-08-25T15:13:57Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62/2020-08-25T151357Z_ImageEncoder
2020-08-25T15:13:57Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62/2020-08-25T151357Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151357Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62/2020-08-25T151357Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/2ff8827bdaf540aa94b800f9f689ef62/2020-08-25T151357Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.1, expected_num_reduced_features = 1
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:58Z INFO     Creating the default output folder structure.
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase/logs
2020-08-25T15:13:58Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:58Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:58Z INFO     Processing dataset (name=None)
2020-08-25T15:13:58Z INFO     Creating the default output folder structure.
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder/logs
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656/2020-08-25T151358Z_ImageEncoder
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656/2020-08-25T151358Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656/2020-08-25T151358Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a35bcc5503eb4b8088f5b8b976901656/2020-08-25T151358Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0')
encode_channels_jointly = True, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1235, -0.0030, -0.1717,  0.2137,  0.1634,  0.2444, -0.0457],
       requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:58Z INFO     Creating the default output folder structure.
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase/logs
2020-08-25T15:13:58Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:58Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:58Z INFO     Processing dataset (name=None)
2020-08-25T15:13:58Z INFO     Creating the default output folder structure.
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder/logs
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0/2020-08-25T151358Z_ImageEncoder
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0/2020-08-25T151358Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0/2020-08-25T151358Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/cdb3e707476d48a79270e5089fe942f0/2020-08-25T151358Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f')
encode_channels_jointly = False, use_non_imaging_features = True
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 0.5, expected_num_reduced_features = 4
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:58Z INFO     Creating the default output folder structure.
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase/logs
2020-08-25T15:13:58Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:58Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:58Z INFO     Processing dataset (name=None)
2020-08-25T15:13:58Z INFO     Creating the default output folder structure.
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder/logs
2020-08-25T15:13:58Z INFO     Running outside of AzureML.
2020-08-25T15:13:58Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f
2020-08-25T15:13:58Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f/2020-08-25T151358Z_ImageEncoder
2020-08-25T15:13:58Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f/2020-08-25T151358Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151358Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f/2020-08-25T151358Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dc2648d8395341d3ad2eefbfebf0cb9f/2020-08-25T151358Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-True] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063')
encode_channels_jointly = True, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2010, -0.2849, -0.2551,  0.1178], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:59Z INFO     Creating the default output folder structure.
2020-08-25T15:13:59Z INFO     Running outside of AzureML.
2020-08-25T15:13:59Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:59Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase
2020-08-25T15:13:59Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase/logs
2020-08-25T15:13:59Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:59Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:59Z INFO     Processing dataset (name=None)
2020-08-25T15:13:59Z INFO     Creating the default output folder structure.
2020-08-25T15:13:59Z INFO     Running outside of AzureML.
2020-08-25T15:13:59Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:59Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder
2020-08-25T15:13:59Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder/logs
2020-08-25T15:13:59Z INFO     Running outside of AzureML.
2020-08-25T15:13:59Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063
2020-08-25T15:13:59Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063/2020-08-25T151359Z_ImageEncoder
2020-08-25T15:13:59Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063/2020-08-25T151359Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063/2020-08-25T151359Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/62e1e1a2049847bfa10a81120a035063/2020-08-25T151359Z_ImageEncoder/logs
_ test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-False] _

test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063')
encode_channels_jointly = False, use_non_imaging_features = False
kernel_size_per_encoding_block = [(1, 1, 1), (1, 3, 3), (3, 3, 3)]
stride_size_per_encoding_block = [(1, 1, 1), (1, 2, 2), (2, 2, 2)]
reduction_factor = 1, expected_num_reduced_features = 0
aggregation_type = <AggregationType.GatedPooling: 'Gated'>

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize(["use_non_imaging_features", "reduction_factor", "expected_num_reduced_features"],
                             [(True, 1, 8), (True, 0.1, 1), (True, 0.5, 4), (False, 1, 0)])
    @pytest.mark.parametrize("kernel_size_per_encoding_block", [None, [(1, 1, 1), (1, 3, 3), (3, 3, 3)]])
    @pytest.mark.parametrize("stride_size_per_encoding_block", [None, [(1, 1, 1), (1, 2, 2), (2, 2, 2)]])
    @pytest.mark.parametrize("aggregation_type", [AggregationType.Average,
                                                  AggregationType.ZAdaptive3dAvg,
                                                  AggregationType.GatedPooling])
    def test_image_encoder(test_output_dirs: TestOutputDirectories, encode_channels_jointly: bool,
                           use_non_imaging_features: bool,
                           kernel_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           stride_size_per_encoding_block: Optional[Union[TupleInt3, List[TupleInt3]]],
                           reduction_factor: float,
                           expected_num_reduced_features: int,
                           aggregation_type: AggregationType) -> None:
        """
        Test if the image encoder networks can be trained without errors (including GradCam computation and data
        augmentation).
        """
        logging_to_stdout()
        set_random_seed(0)
        dataset_folder = Path(test_output_dirs.make_sub_dir("dataset"))
        scan_size = (6, 64, 60)
        scan_files: List[str] = []
        for s in range(4):
            random_scan = np.random.uniform(0, 1, scan_size)
            scan_file_name = f"scan{s + 1}{NumpyFile.Numpy.value}"
            np.save(str(dataset_folder / scan_file_name), random_scan)
            scan_files.append(scan_file_name)
    
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
    S1,week0,scan1.npy,,1,10,Male,Val1
    S1,week1,scan2.npy,True,2,20,Female,Val2
    S2,week0,scan3.npy,,3,30,Female,Val3
    S2,week1,scan4.npy,False,4,40,Female,Val1
    S3,week0,scan1.npy,,5,50,Male,Val2
    S3,week1,scan3.npy,True,6,60,Male,Val2
    """
        (dataset_folder / "dataset.csv").write_text(dataset_contents)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
        config_for_dataset = ScalarModelBase(
            local_dataset=dataset_folder,
            image_channels=["week0", "week1"],
            image_file_column="path",
            label_channels=["week1"],
            label_value_column="label",
            non_image_feature_channels=non_image_feature_channels,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            should_validate=False
        )
        config_for_dataset.read_dataset_into_dataframe_and_pre_process()
    
        dataset = ScalarDataset(config_for_dataset,
                                sample_transforms=ScalarItemAugmentation(
                                    RandAugmentSlice(is_transformation_for_segmentation_maps=False)))
        assert len(dataset) == 3
    
        config = ImageEncoder(
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=config_for_dataset.categorical_feature_encoder,
            encoder_dimensionality_reduction_factor=reduction_factor,
            aggregation_type=aggregation_type,
            scan_size=(6, 64, 60)
        )
    
        if kernel_size_per_encoding_block:
            config.kernel_size_per_encoding_block = kernel_size_per_encoding_block
        if stride_size_per_encoding_block:
            config.stride_size_per_encoding_block = stride_size_per_encoding_block
    
        config.set_output_to(test_output_dirs.root_dir)
        config.max_batch_grad_cam = 1
        model = config.create_model()
        input_size: List[Tuple] = [(len(config.image_channels), *scan_size)]
        if use_non_imaging_features:
            input_size.append((config.get_total_number_of_non_imaging_features(),))
    
            # Original number output channels (unreduced) is
            # num initial channel * (num encoder block - 1) = 4 * (3-1) = 8
            if encode_channels_jointly:
                # reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
            else:
                # num_img_channels * reduced_num_channels + num_non_img_features
                assert model.final_num_feature_channels == len(config.image_channels) * expected_num_reduced_features + \
                       config.get_total_number_of_non_imaging_features()
    
        summarizer = ModelSummary(model)
>       summarizer.generate_summary(input_sizes=input_size)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/model_summary.py:83: in generate_summary
    self._generate_summary(input_tensors)
InnerEye/ML/visualizers/model_summary.py:192: in _generate_summary
    forward_preserve_state(self.model, input_tensors)  # type: ignore
InnerEye/ML/visualizers/model_summary.py:226: in forward_preserve_state
    output = module.forward(*inputs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchprof/profile.py:74: in wrap_forward
    res = _forward(*args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-3.0813]]]],



        [[[[ 0.8039]]]],



        [[[[-1.5337]]]],



        [[[[-1.9779]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[ 2.3483e-02,  5.5411e-02,  6.2503e-02],
           [-2.4245e-02,  1.2159e-02, -3.00...-1.3965e-01,  1.6327e-03, -2.7881e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063
----------------------------- Captured stdout call -----------------------------
Setting logging level to 20
2020-08-25T15:13:59Z INFO     Creating the default output folder structure.
2020-08-25T15:13:59Z INFO     Running outside of AzureML.
2020-08-25T15:13:59Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:59Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase
2020-08-25T15:13:59Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase/logs
2020-08-25T15:13:59Z INFO     Before filtering: 6 rows, 3 unique subjects. 
2020-08-25T15:13:59Z INFO     Final: 6 rows, 3 unique subjects. 
2020-08-25T15:13:59Z INFO     Processing dataset (name=None)
2020-08-25T15:13:59Z INFO     Creating the default output folder structure.
2020-08-25T15:13:59Z INFO     Running outside of AzureML.
2020-08-25T15:13:59Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:13:59Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder
2020-08-25T15:13:59Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder/logs
2020-08-25T15:13:59Z INFO     Running outside of AzureML.
2020-08-25T15:13:59Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063
2020-08-25T15:13:59Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063/2020-08-25T151359Z_ImageEncoder
2020-08-25T15:13:59Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063/2020-08-25T151359Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ScalarModelBase/logs
INFO     root:scalar_config.py:292 Before filtering: 6 rows, 3 unique subjects. 
INFO     root:scalar_config.py:297 Final: 6 rows, 3 unique subjects. 
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151359Z_ImageEncoder/logs
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063/2020-08-25T151359Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/948476f01ff24166acf9be657a4d7063/2020-08-25T151359Z_ImageEncoder/logs
___ test_visualization_with_scalar_model[ImagingFeatureType.Image-True-True] ___

use_non_imaging_features = True
imaging_feature_type = <ImagingFeatureType.Image: 'Image'>
encode_channels_jointly = True
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:309: in generate
    self.forward(*[input[0], self.non_image_input])
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:232: in forward
    x = self.image_and_non_image_features_aggregator(x, item[1].float())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:39: in forward
    x = torch.cat([image_features.flatten(1), non_image_features], dim=1)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seq = [tensor([[ 4.2497,  4.4892, -2.7936, -0.3071, -0.1836, -0.4083],
        [ 4.2497,  4.4892, -2.7936, -0.3071, -0.1836,....,  0.],
        [ 0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.]],
       device='cuda:0', requires_grad=True)]
args = (), kwargs = {'dim': 1}, types = {'FloatTensor'}

    @functools.wraps(orig_fn)
    def wrapper(seq, *args, **kwargs):
        if not _amp_state.handle.is_active():
            return orig_fn(seq, *args, **kwargs)
    
        types = set([utils.type_string(x) for x in seq])
        if len(types) <= 1:
>           return orig_fn(seq, *args, **kwargs)
E           RuntimeError: All input tensors must be on the same device. Received cpu and cuda:0

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: RuntimeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:07Z INFO     Creating the default output folder structure.
2020-08-25T15:14:07Z INFO     Running outside of AzureML.
2020-08-25T15:14:07Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:07Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151407Z_ImageEncoder
2020-08-25T15:14:07Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151407Z_ImageEncoder/logs
2020-08-25T15:14:07Z INFO     Processing dataset (name=None)
2020-08-25T15:14:07Z INFO     Running outside of AzureML.
2020-08-25T15:14:07Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e
2020-08-25T15:14:07Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e/2020-08-25T151407Z_ImageEncoder
2020-08-25T15:14:07Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e/2020-08-25T151407Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151407Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151407Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e/2020-08-25T151407Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/26f5d3a775fc47e7817dcd940d2c7d9e/2020-08-25T151407Z_ImageEncoder/logs
__ test_visualization_with_scalar_model[ImagingFeatureType.Image-True-False] ___

use_non_imaging_features = True
imaging_feature_type = <ImagingFeatureType.Image: 'Image'>
encode_channels_jointly = False
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:309: in generate
    self.forward(*[input[0], self.non_image_input])
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.5978, -0.4302,  0.2264],
           [-0.1954, -0.8067,  0.1718],
           [-0.2...1236,  1.2125],
           [-0.2241,  0.1705, -0.2731],
           [-0.3420,  0.6508,  0.8202]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:08Z INFO     Creating the default output folder structure.
2020-08-25T15:14:08Z INFO     Running outside of AzureML.
2020-08-25T15:14:08Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:08Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder
2020-08-25T15:14:08Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder/logs
2020-08-25T15:14:08Z INFO     Processing dataset (name=None)
2020-08-25T15:14:08Z INFO     Running outside of AzureML.
2020-08-25T15:14:08Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4
2020-08-25T15:14:08Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4/2020-08-25T151408Z_ImageEncoder
2020-08-25T15:14:08Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4/2020-08-25T151408Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4/2020-08-25T151408Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ceb015a743654972bcfb300f7125c7e4/2020-08-25T151408Z_ImageEncoder/logs
__ test_visualization_with_scalar_model[ImagingFeatureType.Image-False-True] ___

use_non_imaging_features = False
imaging_feature_type = <ImagingFeatureType.Image: 'Image'>
encode_channels_jointly = True
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:314: in generate
    self.forward(*input)
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.2188,  0.1061, -0.2604, -0.3027], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:08Z INFO     Creating the default output folder structure.
2020-08-25T15:14:08Z INFO     Running outside of AzureML.
2020-08-25T15:14:08Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:08Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder
2020-08-25T15:14:08Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder/logs
2020-08-25T15:14:08Z INFO     Processing dataset (name=None)
2020-08-25T15:14:08Z INFO     Running outside of AzureML.
2020-08-25T15:14:08Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd
2020-08-25T15:14:08Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd/2020-08-25T151408Z_ImageEncoder
2020-08-25T15:14:08Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd/2020-08-25T151408Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151408Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd/2020-08-25T151408Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/5f1f085a77e2492693805d5be43fdcdd/2020-08-25T151408Z_ImageEncoder/logs
__ test_visualization_with_scalar_model[ImagingFeatureType.Image-False-False] __

use_non_imaging_features = False
imaging_feature_type = <ImagingFeatureType.Image: 'Image'>
encode_channels_jointly = False
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:314: in generate
    self.forward(*input)
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-0.0826,  0.2140,  0.5672],
           [ 0.4163, -0.6822,  0.6950],
           [ 0.2...1655,  0.0970],
           [ 0.2301, -0.4517, -0.8087],
           [-0.0526,  0.2654, -0.4119]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:09Z INFO     Creating the default output folder structure.
2020-08-25T15:14:09Z INFO     Running outside of AzureML.
2020-08-25T15:14:09Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:09Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder
2020-08-25T15:14:09Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder/logs
2020-08-25T15:14:09Z INFO     Processing dataset (name=None)
2020-08-25T15:14:09Z INFO     Running outside of AzureML.
2020-08-25T15:14:09Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442
2020-08-25T15:14:09Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442/2020-08-25T151409Z_ImageEncoder
2020-08-25T15:14:09Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442/2020-08-25T151409Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442/2020-08-25T151409Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/a519533ab6bc4f63adba440804e88442/2020-08-25T151409Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-True-True] _

use_non_imaging_features = True
imaging_feature_type = <ImagingFeatureType.Segmentation: 'Segmentation'>
encode_channels_jointly = True
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:309: in generate
    self.forward(*[input[0], self.non_image_input])
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:232: in forward
    x = self.image_and_non_image_features_aggregator(x, item[1].float())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:39: in forward
    x = torch.cat([image_features.flatten(1), non_image_features], dim=1)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seq = [tensor([[ 0.2113,  0.0390, -0.0840,  0.1749,  0.0894,  0.2571],
        [ 0.2113,  0.0390, -0.0840,  0.1749,  0.0894,....,  0.],
        [ 0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.]],
       device='cuda:0', requires_grad=True)]
args = (), kwargs = {'dim': 1}, types = {'FloatTensor'}

    @functools.wraps(orig_fn)
    def wrapper(seq, *args, **kwargs):
        if not _amp_state.handle.is_active():
            return orig_fn(seq, *args, **kwargs)
    
        types = set([utils.type_string(x) for x in seq])
        if len(types) <= 1:
>           return orig_fn(seq, *args, **kwargs)
E           RuntimeError: All input tensors must be on the same device. Received cpu and cuda:0

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: RuntimeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:09Z INFO     Creating the default output folder structure.
2020-08-25T15:14:09Z INFO     Running outside of AzureML.
2020-08-25T15:14:09Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:09Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder
2020-08-25T15:14:09Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder/logs
2020-08-25T15:14:09Z INFO     Processing dataset (name=None)
2020-08-25T15:14:09Z INFO     Running outside of AzureML.
2020-08-25T15:14:09Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94
2020-08-25T15:14:09Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94/2020-08-25T151409Z_ImageEncoder
2020-08-25T15:14:09Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94/2020-08-25T151409Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94/2020-08-25T151409Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/8acfcb3420084004b36368ac17739b94/2020-08-25T151409Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-True-False] _

use_non_imaging_features = True
imaging_feature_type = <ImagingFeatureType.Segmentation: 'Segmentation'>
encode_channels_jointly = False
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:309: in generate
    self.forward(*[input[0], self.non_image_input])
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 0.2951, -0.0447,  0.1372],
           [-0.1481,  0.0565,  0.2375],
           [ 0.1...1613,  0.1463],
           [ 0.0404,  0.0373, -0.2357],
           [ 0.2176, -0.0422, -0.0009]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:09Z INFO     Creating the default output folder structure.
2020-08-25T15:14:09Z INFO     Running outside of AzureML.
2020-08-25T15:14:09Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:09Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder
2020-08-25T15:14:09Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder/logs
2020-08-25T15:14:09Z INFO     Processing dataset (name=None)
2020-08-25T15:14:09Z INFO     Running outside of AzureML.
2020-08-25T15:14:09Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5
2020-08-25T15:14:09Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5/2020-08-25T151409Z_ImageEncoder
2020-08-25T15:14:09Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5/2020-08-25T151409Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151409Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5/2020-08-25T151409Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/dd62408dfa474caa90b3c2936c0d36b5/2020-08-25T151409Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-False-True] _

use_non_imaging_features = False
imaging_feature_type = <ImagingFeatureType.Segmentation: 'Segmentation'>
encode_channels_jointly = True
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:314: in generate
    self.forward(*input)
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([ 0.2553, -0.0693, -0.3265, -0.1951], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:10Z INFO     Creating the default output folder structure.
2020-08-25T15:14:10Z INFO     Running outside of AzureML.
2020-08-25T15:14:10Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:10Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder
2020-08-25T15:14:10Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder/logs
2020-08-25T15:14:10Z INFO     Processing dataset (name=None)
2020-08-25T15:14:10Z INFO     Running outside of AzureML.
2020-08-25T15:14:10Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647
2020-08-25T15:14:10Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647/2020-08-25T151410Z_ImageEncoder
2020-08-25T15:14:10Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647/2020-08-25T151410Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647/2020-08-25T151410Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/84ad4f8936e6431a9087401c70254647/2020-08-25T151410Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-False-False] _

use_non_imaging_features = False
imaging_feature_type = <ImagingFeatureType.Segmentation: 'Segmentation'>
encode_channels_jointly = False
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:314: in generate
    self.forward(*input)
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 1.1471e-01, -1.3988e-01, -1.7114e-01],
           [-1.3168e-01, -5.8538e-02, -6.065...   [-1.3382e-01, -1.1645e-01, -1.9567e-01],
           [-1.7411e-01,  2.9162e-02,  1.7144e-01]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:10Z INFO     Creating the default output folder structure.
2020-08-25T15:14:10Z INFO     Running outside of AzureML.
2020-08-25T15:14:10Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:10Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder
2020-08-25T15:14:10Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder/logs
2020-08-25T15:14:10Z INFO     Processing dataset (name=None)
2020-08-25T15:14:10Z INFO     Running outside of AzureML.
2020-08-25T15:14:10Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017
2020-08-25T15:14:10Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017/2020-08-25T151410Z_ImageEncoder
2020-08-25T15:14:10Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017/2020-08-25T151410Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017/2020-08-25T151410Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/fd1a46192ee04db9a573755c85921017/2020-08-25T151410Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-True-True] _

use_non_imaging_features = True
imaging_feature_type = <ImagingFeatureType.ImageAndSegmentation: 'ImageAndSegmentation'>
encode_channels_jointly = True
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:309: in generate
    self.forward(*[input[0], self.non_image_input])
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:232: in forward
    x = self.image_and_non_image_features_aggregator(x, item[1].float())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:39: in forward
    x = torch.cat([image_features.flatten(1), non_image_features], dim=1)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: in wrapper
    return orig_fn(seq, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seq = [tensor([[-0.0444, -0.1829,  0.0599,  0.1698, -0.5410,  0.0025],
        [-0.0444, -0.1829,  0.0599,  0.1698, -0.5410,....,  0.],
        [ 0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.]],
       device='cuda:0', requires_grad=True)]
args = (), kwargs = {'dim': 1}, types = {'FloatTensor'}

    @functools.wraps(orig_fn)
    def wrapper(seq, *args, **kwargs):
        if not _amp_state.handle.is_active():
            return orig_fn(seq, *args, **kwargs)
    
        types = set([utils.type_string(x) for x in seq])
        if len(types) <= 1:
>           return orig_fn(seq, *args, **kwargs)
E           RuntimeError: All input tensors must be on the same device. Received cpu and cuda:0

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:81: RuntimeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:10Z INFO     Creating the default output folder structure.
2020-08-25T15:14:10Z INFO     Running outside of AzureML.
2020-08-25T15:14:10Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:10Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder
2020-08-25T15:14:10Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder/logs
2020-08-25T15:14:10Z INFO     Processing dataset (name=None)
2020-08-25T15:14:11Z INFO     Running outside of AzureML.
2020-08-25T15:14:11Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6
2020-08-25T15:14:11Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6/2020-08-25T151411Z_ImageEncoder
2020-08-25T15:14:11Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6/2020-08-25T151411Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151410Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6/2020-08-25T151411Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/ead90ebd1e3349d092872c0dab897eb6/2020-08-25T151411Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-True-False] _

use_non_imaging_features = True
imaging_feature_type = <ImagingFeatureType.ImageAndSegmentation: 'ImageAndSegmentation'>
encode_channels_jointly = False
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:309: in generate
    self.forward(*[input[0], self.non_image_input])
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[ 6.9974e-02, -2.7432e-02, -1.7476e-01],
           [-5.5249e-02, -1.1433e-01, -1.120...   [ 7.3020e-03, -2.0708e-01,  7.5563e-02],
           [ 1.2052e-01, -1.3901e-01,  1.8624e-01]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:11Z INFO     Creating the default output folder structure.
2020-08-25T15:14:11Z INFO     Running outside of AzureML.
2020-08-25T15:14:11Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:11Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151411Z_ImageEncoder
2020-08-25T15:14:11Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151411Z_ImageEncoder/logs
2020-08-25T15:14:11Z INFO     Processing dataset (name=None)
2020-08-25T15:14:11Z INFO     Running outside of AzureML.
2020-08-25T15:14:11Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c
2020-08-25T15:14:11Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c/2020-08-25T151411Z_ImageEncoder
2020-08-25T15:14:11Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c/2020-08-25T151411Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151411Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151411Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c/2020-08-25T151411Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/d03e92234ddd4bbdb4d9ff73e5f6e58c/2020-08-25T151411Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-False-True] _

use_non_imaging_features = False
imaging_feature_type = <ImagingFeatureType.ImageAndSegmentation: 'ImageAndSegmentation'>
encode_channels_jointly = True
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:314: in generate
    self.forward(*input)
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:336: in forward
    x = self.classification_layer(x.view(-1, x.shape[1]))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:57: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/mlp.py:50: in forward
    return self.model(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/linear.py:91: in forward
    return F.linear(input, self.weight, self.bias)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:28: in wrapper
    return orig_fn(*new_args, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1674: in linear
    ret = torch.addmm(bias, input, weight.t())
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([-0.1782,  0.2156, -0.1345, -0.1743], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:12Z INFO     Creating the default output folder structure.
2020-08-25T15:14:12Z INFO     Running outside of AzureML.
2020-08-25T15:14:12Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:12Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder
2020-08-25T15:14:12Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder/logs
2020-08-25T15:14:12Z INFO     Processing dataset (name=None)
2020-08-25T15:14:12Z INFO     Running outside of AzureML.
2020-08-25T15:14:12Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534
2020-08-25T15:14:12Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534/2020-08-25T151412Z_ImageEncoder
2020-08-25T15:14:12Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534/2020-08-25T151412Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534/2020-08-25T151412Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/baab3b58368540cd9eecc3e113db9534/2020-08-25T151412Z_ImageEncoder/logs
_ test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-False-False] _

use_non_imaging_features = False
imaging_feature_type = <ImagingFeatureType.ImageAndSegmentation: 'ImageAndSegmentation'>
encode_channels_jointly = False
test_output_dirs = TestOutputDirectories(root_dir='/home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24')

    @pytest.mark.skipif(common_util.is_windows(), reason="Too slow on windows")
    @pytest.mark.parametrize("encode_channels_jointly", [True, False])
    @pytest.mark.parametrize("use_non_imaging_features", [True, False])
    @pytest.mark.parametrize("imaging_feature_type", [ImagingFeatureType.Image,
                                                      ImagingFeatureType.Segmentation,
                                                      ImagingFeatureType.ImageAndSegmentation])
    def test_visualization_with_scalar_model(use_non_imaging_features: bool,
                                             imaging_feature_type: ImagingFeatureType,
                                             encode_channels_jointly: bool,
                                             test_output_dirs: TestOutputDirectories) -> None:
        dataset_contents = """subject,channel,path,label,numerical1,numerical2,categorical1,categorical2
        S1,week0,scan1.npy,,1,10,Male,Val1
        S1,week1,scan2.npy,True,2,20,Female,Val2
        S2,week0,scan3.npy,,3,30,Female,Val3
        S2,week1,scan4.npy,False,4,40,Female,Val1
        S3,week0,scan1.npy,,5,50,Male,Val2
        S3,week1,scan3.npy,True,6,60,Male,Val2
        """
        dataset_dataframe = pd.read_csv(StringIO(dataset_contents), dtype=str)
        numerical_columns = ["numerical1", "numerical2"] if use_non_imaging_features else []
        categorical_columns = ["categorical1", "categorical2"] if use_non_imaging_features else []
        non_image_feature_channels = get_non_image_features_dict(default_channels=["week1", "week0"],
                                                                 specific_channels={"categorical2": ["week1"]}) \
            if use_non_imaging_features else {}
    
        config = ImageEncoder(
            local_dataset=Path(),
            encode_channels_jointly=encode_channels_jointly,
            should_validate=False,
            numerical_columns=numerical_columns,
            categorical_columns=categorical_columns,
            imaging_feature_type=imaging_feature_type,
            non_image_feature_channels=non_image_feature_channels,
            categorical_feature_encoder=CategoricalToOneHotEncoder.create_from_dataframe(
                dataframe=dataset_dataframe, columns=categorical_columns)
        )
    
        dataloader = ScalarDataset(config, data_frame=dataset_dataframe) \
            .as_data_loader(shuffle=False, batch_size=2)
    
        config.set_output_to(test_output_dirs.root_dir)
        config.num_epochs = 1
        model = config.create_model()
        # Patch the load_images function that will be called once we access a dataset item
        image_and_seg = ImageAndSegmentations[np.ndarray](images=np.random.uniform(0, 1, (6, 64, 60)),
                                                          segmentations=np.random.randint(0, 2, (6, 64, 60)))
        with mock.patch('InnerEye.ML.utils.io_util.load_image_in_known_formats', return_value=image_and_seg):
            batch = next(iter(dataloader))
            model_inputs_and_labels = get_scalar_model_inputs_and_labels(config, model, batch)
    
        number_channels = len(config.image_channels)
        number_subjects = len(model_inputs_and_labels.subject_ids)
        visualizer = VisualizationMaps(model, config)
        guided_grad_cams, grad_cams, pseudo_cam_non_img, probas = visualizer.generate(
>           model_inputs_and_labels.model_inputs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
InnerEye/ML/visualizers/grad_cam_hooks.py:463: in generate
    image_gcam, pseudo_cam_non_img, probability = self.grad_cam.generate(input, target_position, target_label_index)
InnerEye/ML/visualizers/grad_cam_hooks.py:314: in generate
    self.forward(*input)
InnerEye/ML/visualizers/grad_cam_hooks.py:165: in forward
    self.logits = self.model(*input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:334: in forward
    x = super().forward(*item)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:228: in forward
    x = self.encode_and_aggregate(x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:242: in encode_and_aggregate
    input_tensor=x)
InnerEye/ML/models/architectures/classification/image_encoder_with_mlp.py:363: in encode_and_aggregate
    encoder_output = encoder(input_tensor[:, start_index:end_index].view(channel_shape))
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/container.py:117: in forward
    input = module(input)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/architectures/unet_3d.py:150: in forward
    x = self.block1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
InnerEye/ML/models/layers/basic.py:61: in forward
    x = self.conv1(x)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/module.py:722: in _call_impl
    result = self.forward(*input, **kwargs)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/modules/conv.py:567: in forward
    self.padding, self.dilation, self.groups)
../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/wrap.py:21: in wrapper
    args[i] = utils.cached_cast(cast_fn, args[i], handle.cache)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cast_fn = <function maybe_half at 0x7fe1ad2e6048>
x = Parameter containing:
tensor([[[[[-1.9306e-01, -7.7629e-03, -2.5591e-02],
           [ 6.0180e-02, -3.9699e-03,  3.727...   [-4.2565e-02,  2.8259e-02,  5.0788e-02],
           [-2.8087e-02,  3.3705e-03,  3.8177e-01]]]]], requires_grad=True)
cache = {Parameter containing:
tensor([[[[[-4.5233e-02,  1.7275e-01,  2.0183e-01],
           [-7.7676e-03,  3.5107e-03, -1.51... 3.3051e-02, -3.8965e-01, -2.6489e-01]]]]], device='cuda:0',
       dtype=torch.float16, grad_fn=<CopyBackwards>), ...}

    def cached_cast(cast_fn, x, cache):
        if is_nested(x):
            return type(x)([cached_cast(y) for y in x])
        if x in cache:
            cached_x = cache[x]
            if x.requires_grad and cached_x.requires_grad:
                # Make sure x is actually cached_x's autograd parent.
>               if cached_x.grad_fn.next_functions[1][0].variable is not x:
E               AttributeError: 'NoneType' object has no attribute 'next_functions'

../miniconda3/envs/InnerEye/lib/python3.7/site-packages/apex/amp/utils.py:97: AttributeError
---------------------------- Captured stdout setup -----------------------------
Created temporary folder for test: /home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24
----------------------------- Captured stdout call -----------------------------
2020-08-25T15:14:12Z INFO     Creating the default output folder structure.
2020-08-25T15:14:12Z INFO     Running outside of AzureML.
2020-08-25T15:14:12Z INFO     All results will be written to a subfolder of the project root folder.
2020-08-25T15:14:12Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder
2020-08-25T15:14:12Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder/logs
2020-08-25T15:14:12Z INFO     Processing dataset (name=None)
2020-08-25T15:14:12Z INFO     Running outside of AzureML.
2020-08-25T15:14:12Z INFO     All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24
2020-08-25T15:14:12Z INFO     Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24/2020-08-25T151412Z_ImageEncoder
2020-08-25T15:14:12Z INFO     Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24/2020-08-25T151412Z_ImageEncoder/logs
------------------------------ Captured log call -------------------------------
INFO     root:deep_learning_config.py:320 Creating the default output folder structure.
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:111 All results will be written to a subfolder of the project root folder.
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/outputs/2020-08-25T151412Z_ImageEncoder/logs
INFO     root:full_image_dataset.py:172 Processing dataset (name=None)
INFO     root:deep_learning_config.py:106 Running outside of AzureML.
INFO     root:deep_learning_config.py:108 All results will be written to the specified output folder /home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24
INFO     root:deep_learning_config.py:123 Run outputs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24/2020-08-25T151412Z_ImageEncoder
INFO     root:deep_learning_config.py:124 Logs folder: /home/dacart/InnerEye-DeepLearning/test_outputs/94c52474b36a47658ff58d7e03777b24/2020-08-25T151412Z_ImageEncoder/logs
=============================== warnings summary ===============================
/home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/azureml/data/dataset_type_definitions.py:69
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/azureml/data/dataset_type_definitions.py:69: DeprecationWarning: FileType Enum is Deprecated in > 1.0.39. Use strings instead.
    category=DeprecationWarning)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py: 17 warnings
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:805: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless
    UndefinedMetricWarning)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py: 18 warnings
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/sklearn/metrics/_ranking.py:813: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless
    UndefinedMetricWarning)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py: 12 warnings
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
    "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-1-8-True]
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=trilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
    "See the documentation of nn.Upsample for details.".format(mode))

Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-1-8-True]
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/ansiwrap/core.py:6: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
    import imp

Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-1-8-True]
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/traitlets/config/configurable.py:73: DeprecationWarning: Passing unrecoginized arguments to super(PapermillNotebookClient).__init__(input_path='/home/dacart/InnerEye-DeepLearning/InnerEye/ML/visualizers/gradcam_visualization.ipynb').
  object.__init__() takes exactly one argument (the instance to initialize)
  This is deprecated in traitlets 4.2.This error will be raised in a future release of traitlets.
    super(Configurable, self).__init__(**kwargs)

Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder_with_segmentation[ImagingFeatureType.Segmentation-AggregationType.Average-True]
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/tensor_numpy.cpp:141.)
    img = torch.from_numpy(np.array(pic, np.int32, copy=False))

Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder_with_segmentation[ImagingFeatureType.Segmentation-AggregationType.MixPooling-True]
Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder_with_segmentation[ImagingFeatureType.Segmentation-AggregationType.MixPooling-False]
Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder_with_segmentation[ImagingFeatureType.ImageAndSegmentation-AggregationType.MixPooling-True]
Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder_with_segmentation[ImagingFeatureType.ImageAndSegmentation-AggregationType.MixPooling-False]
  /home/dacart/miniconda3/envs/InnerEye/lib/python3.7/site-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
    warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-None-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-None-kernel_size_per_encoding_block1-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-None-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.Average-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-None-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-None-kernel_size_per_encoding_block1-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-None-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.ZAdaptive3dAvg-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-None-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-None-kernel_size_per_encoding_block1-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-None-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-1-8-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.1-1-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-True-0.5-4-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_image_encoder[AggregationType.GatedPooling-stride_size_per_encoding_block1-kernel_size_per_encoding_block1-False-1-0-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Image-True-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Image-True-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Image-False-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Image-False-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-True-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-True-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-False-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.Segmentation-False-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-True-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-True-False]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-False-True]
FAILED Tests/ML/models/architectures/test_image_encoder_with_mlp.py::test_visualization_with_scalar_model[ImagingFeatureType.ImageAndSegmentation-False-False]
============ 107 failed, 21 passed, 56 warnings in 72.90s (0:01:12) ============
